---
apiVersion: v1
kind: ConfigMap
metadata:
  name: training-config-qlora
  namespace: mlflow
data:
  axolotl_config.yaml: |
    # Axolotl configuration for QLoRA training
    base_model: Qwen/Qwen2.5-7B-Instruct
    model_type: AutoModelForCausalLM
    tokenizer_type: AutoTokenizer
    trust_remote_code: true
    
    # Dataset
    datasets:
      - path: Team-ACE/ToolACE
        type: chat_template  # Updated from deprecated 'sharegpt'
        chat_template: chatml
        field_messages: conversations
        message_field_role: from
        message_field_content: value
    
    # Training settings
    sequence_len: 2048
    sample_packing: true
    pad_to_sequence_len: true
    
    # QLoRA configuration
    adapter: qlora
    lora_r: 64
    lora_alpha: 128
    lora_dropout: 0.05
    lora_target_linear: true
    
    # Quantization
    load_in_4bit: true
    bnb_4bit_quant_type: nf4
    bnb_4bit_compute_dtype: bfloat16
    bnb_4bit_use_double_quant: true
    
    # Optimizations
    flash_attention: true
    bf16: true
    tf32: true
    gradient_checkpointing: true
    
    # Hyperparameters
    num_epochs: 3
    micro_batch_size: 2
    gradient_accumulation_steps: 8
    learning_rate: 2.0e-4
    lr_scheduler: cosine
    warmup_ratio: 0.1
    weight_decay: 0.01
    
    # Evaluation
    val_set_size: 0.05
    eval_steps: 100
    save_steps: 100
    logging_steps: 10
    
    # Output
    output_dir: /mnt/data/outputs/qlora-qwen25-7b
    
    # MLflow integration
    use_mlflow: true

---
apiVersion: batch/v1
kind: Job
metadata:
  name: training-qlora
  namespace: mlflow
  labels:
    job-type: training
    method: qlora
spec:
  backoffLimit: 1
  ttlSecondsAfterFinished: 86400  # Keep for 24 hours after completion
  template:
    metadata:
      labels:
        job: training
        method: qlora
    spec:
      restartPolicy: Never
      
      # Use GPU nodes (with library-solution label)
      nodeSelector:
        library-solution: k8s-training
      
      # Tolerations for GPU nodes (if tainted)
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      
      containers:
      - name: training
        image: pytorch/pytorch:2.5.1-cuda12.1-cudnn9-devel  # Official PyTorch with CUDA
        
        command:
        - /bin/bash
        - -c
        - |
          set -ex
          
          export MLFLOW_TRACKING_URI=http://mlflow-server.mlflow.svc.cluster.local:5000
          export MLFLOW_EXPERIMENT_NAME=fintech-function-calling
          export DEBIAN_FRONTEND=noninteractive
          
          echo "Installing system dependencies..."
          apt-get update && apt-get install -y git build-essential
          
          echo "Installing Axolotl from PyPI (per official docs)..."
          pip install packaging setuptools wheel ninja
          pip install --no-build-isolation 'axolotl[flash-attn,deepspeed]'
          
          # Install mlflow for experiment tracking
          echo "Installing mlflow..."
          pip install mlflow
          
          # Create triton cache directory
          mkdir -p /root/.triton/autotune
          
          # Create workspace directory
          mkdir -p /workspace
          cd /workspace
          
          # Copy config
          cp /config/axolotl_config.yaml ./axolotl_config.yaml
          
          # Create outputs directory on filestore
          mkdir -p /mnt/data/outputs
          
          echo "Starting QLoRA training..."
          echo "Axolotl installed at: $(pip show axolotl | grep Location)"
          
          # Run training using the installed CLI
          echo "Running: accelerate launch -m axolotl.cli.train axolotl_config.yaml"
          accelerate launch -m axolotl.cli.train axolotl_config.yaml
          
          echo "Training completed!"
          
          # Copy final model to MLflow artifacts location
          if [ -d "/mnt/data/outputs/qlora-qwen25-7b" ]; then
            echo "Model saved to /mnt/data/outputs/qlora-qwen25-7b"
          fi
        
        env:
        - name: MLFLOW_TRACKING_URI
          value: "http://mlflow-server.mlflow.svc.cluster.local:5000"
        - name: MLFLOW_EXPERIMENT_NAME
          value: "fintech-function-calling"
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: huggingface-secret
              key: token
              optional: true
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        
        resources:
          requests:
            nvidia.com/gpu: 1
            cpu: 8
            memory: 100Gi
          limits:
            nvidia.com/gpu: 1
            cpu: 16
            memory: 200Gi
        
        volumeMounts:
        - name: config
          mountPath: /config
        - name: data
          mountPath: /mnt/data
        - name: workspace
          mountPath: /workspace
        - name: huggingface-cache
          mountPath: /root/.cache/huggingface
      
      volumes:
      - name: config
        configMap:
          name: training-config-qlora
      - name: data
        # Use hostPath if filestore is mounted at /mnt/data on nodes
        hostPath:
          path: /mnt/data
          type: DirectoryOrCreate
      - name: workspace
        emptyDir: {}
      - name: huggingface-cache
        emptyDir: {}  # For model cache

