# AWS EKS Configuration Example
# Copy this file to terraform.tfvars and fill in your values

# ============================================================================
# AWS Configuration
# ============================================================================
aws_region  = "us-east-1"  # Change to your preferred region
environment = "dev"        # dev, staging, or prod

# ============================================================================
# EKS Cluster Configuration
# ============================================================================
cluster_name = "fintech-llm-cluster"
k8s_version  = "1.28"

# ============================================================================
# VPC Configuration
# ============================================================================
vpc_cidr = "10.0.0.0/16"

# Must have at least 2 AZs for EKS
availability_zones = ["us-east-1a", "us-east-1b"]

# ============================================================================
# CPU Node Configuration
# ============================================================================
cpu_nodes_desired = 1
cpu_nodes_min     = 1
cpu_nodes_max     = 3

# Instance types for CPU nodes
cpu_instance_types = ["t3.xlarge"]  # 4 vCPU, 16GB RAM

# ============================================================================
# GPU Node Configuration
# ============================================================================
gpu_nodes_desired = 1
gpu_nodes_min     = 0  # Can scale to 0 when not training
gpu_nodes_max     = 2

# GPU Instance Types (choose based on your needs and budget):
# - p3.2xlarge:   1x V100 (16GB VRAM),  8 vCPU,  61GB RAM  - ~$3/hr
# - p3.8xlarge:   4x V100 (64GB VRAM),  32 vCPU, 244GB RAM - ~$12/hr
# - p4d.24xlarge: 8x A100 (320GB VRAM), 96 vCPU, 1152GB RAM - ~$32/hr
# - p5.48xlarge:  8x H100 (640GB VRAM), 192 vCPU, 2048GB RAM - ~$98/hr (limited availability)

gpu_instance_types = ["p3.2xlarge"]  # Start with single V100 for demo

gpu_disk_size = 200  # GB

# ============================================================================
# Storage Configuration
# ============================================================================
enable_efs = true  # Enable EFS for shared storage (models, datasets)

# ============================================================================
# SSH Access (Optional)
# ============================================================================
# If you want SSH access to nodes, specify your EC2 key pair name
# ssh_key_name = "my-ec2-keypair"

# ============================================================================
# Tags (Optional)
# ============================================================================
additional_tags = {
  Owner       = "your-name"
  CostCenter  = "ml-engineering"
  Project     = "llm-function-calling"
}

# ============================================================================
# Example Production Configuration
# ============================================================================
# For production with high availability and more GPUs:
#
# environment = "prod"
# 
# # CPU nodes
# cpu_nodes_desired = 2
# cpu_nodes_min     = 2
# cpu_nodes_max     = 5
# cpu_instance_types = ["t3.2xlarge"]  # 8 vCPU, 32GB RAM
#
# # GPU nodes
# gpu_nodes_desired = 2
# gpu_nodes_min     = 1
# gpu_nodes_max     = 4
# gpu_instance_types = ["p4d.24xlarge"]  # 8x A100
#
# # Use Spot instances for cost savings
# # (requires additional configuration)

# ============================================================================
# Cost Optimization Tips
# ============================================================================
# 1. Use Spot Instances:
#    - Can save up to 70% on GPU costs
#    - Suitable for training workloads (can handle interruptions)
#    - Configure with lifecycle: spot = true
#
# 2. Auto-scaling:
#    - Set gpu_nodes_min = 0 to scale down completely when idle
#    - Use Karpenter for advanced auto-scaling
#
# 3. Reserved Instances:
#    - For predictable, long-running workloads
#    - Can save up to 75% vs on-demand
#
# 4. Right-sizing:
#    - Start with p3.2xlarge (1x V100) for testing
#    - Scale up to p4d/p5 only when needed
